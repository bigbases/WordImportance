{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608658f6",
   "metadata": {},
   "source": [
    "# 0. Load required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd8d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2b1d0",
   "metadata": {},
   "source": [
    "# 1. Load tokenizer and dependency extraction module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d0098e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "vocab_file = '/home/skhong/WordImportance/bert/qnli/vocab.txt'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f3945",
   "metadata": {},
   "source": [
    "# 2. Load Dataset\n",
    "- Preprocessed dataset used in this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c35d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_file(\"/home/skhong/.cache/huggingface/datasets/glue/qnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-0295a6411edbdafe.arrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3339d",
   "metadata": {},
   "source": [
    "## 2.1 Dataset Structure\n",
    "- Since the dataset consists of two sentences, namely a premise and a hypothesis, there is an assumption that a [SEP] token is included between the two sentences. \n",
    "- Therefore, it is necessary to extract relationships between words within the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8688839a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(features: {'question': Value(dtype='string', id=None), 'sentence': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['entailment', 'not_entailment'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'tfidf_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}, num_rows: 5463)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25728be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['tfidf_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d80fa743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BertTokenizer' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_max_len\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BertTokenizer' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "dataset['sentence2'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22081d73",
   "metadata": {},
   "source": [
    "# 3. Random_token_select Function\n",
    "- A function that randomly selects one token from tokens excluding tokens with dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a1eadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_value_except(lst, excluded_value):\n",
    "    filtered_list = [item for item in lst if item != excluded_value]\n",
    "\n",
    "    if filtered_list:\n",
    "        random_value = random.choice(filtered_list)\n",
    "        return random_value\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60490e29",
   "metadata": {},
   "source": [
    "# 4. Dataset Generater\n",
    "- Compare tokenization results of two tokenizers, identify sentences with inter-token dependencies, extract token positions within those sentences, as well as positions of tokens without inter-token dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf937434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21110/891669665.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(shuffle_index):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e4dd2b1fda48e1a9d549fdd4cf4fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5463.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n",
      "380\n",
      "400\n",
      "420\n",
      "440\n",
      "460\n",
      "480\n",
      "500\n",
      "520\n",
      "540\n",
      "560\n",
      "580\n",
      "600\n",
      "620\n",
      "640\n",
      "660\n",
      "680\n",
      "700\n",
      "720\n",
      "740\n",
      "760\n",
      "780\n",
      "800\n",
      "820\n",
      "840\n",
      "860\n",
      "880\n",
      "900\n",
      "920\n",
      "940\n",
      "960\n",
      "980\n",
      "1000\n",
      "1020\n",
      "1040\n",
      "1060\n",
      "1080\n",
      "1100\n",
      "1120\n",
      "1140\n",
      "1160\n",
      "1180\n",
      "1200\n",
      "1220\n",
      "1240\n",
      "1260\n",
      "1280\n",
      "1300\n",
      "1320\n",
      "1340\n",
      "1360\n",
      "1380\n",
      "1400\n",
      "1420\n",
      "1440\n",
      "1460\n",
      "1480\n",
      "1500\n",
      "1520\n",
      "1540\n",
      "1560\n",
      "1580\n",
      "1600\n",
      "1620\n",
      "1640\n",
      "1660\n",
      "1680\n",
      "1700\n",
      "1720\n",
      "1740\n",
      "1760\n",
      "1780\n",
      "1800\n",
      "1820\n",
      "1840\n",
      "1860\n",
      "1880\n",
      "1900\n",
      "1920\n",
      "1940\n",
      "1960\n",
      "1980\n",
      "2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pos = []\n",
    "data_set1 = []\n",
    "data_set2 = []\n",
    "\n",
    "shuffle_index = [i for i in range(len(dataset['question']))]\n",
    "random.shuffle(shuffle_index)\n",
    "\n",
    "for i in tqdm_notebook(shuffle_index):    \n",
    "    y_temp = []\n",
    "    text1 = dataset['sentence'][i]\n",
    "    doc1 = nlp(text1)\n",
    "    tokens1_1 = [d.text for d in doc1]\n",
    "    tokens2_1 = tokenizer.tokenize(text1)\n",
    "    \n",
    "    text2 = dataset['question'][i]\n",
    "    doc2 = nlp(text2)\n",
    "    tokens1_2 = [d.text for d in doc2]\n",
    "    tokens2_2 = tokenizer.tokenize(text2)  \n",
    "    \n",
    "    input_ids = torch.Tensor([dataset['input_ids'][i]]).type(torch.int32)\n",
    "    token_type_ids = torch.Tensor([dataset['token_type_ids'][i]]).type(torch.int32)\n",
    "    tfidf_ids = torch.Tensor([dataset['tfidf_ids'][i]]).type(torch.int32)\n",
    "    # \n",
    "    \n",
    "    for token in doc1:\n",
    "        if (token.text in tokens2_1) and (token.head.text in tokens2_1):\n",
    "            random_numbers = [ii+1 for ii in range(len(tokens1_1))]\n",
    "            i_pos = tokens2_1.index(token.text) + 1\n",
    "            j_pos = tokens2_1.index(token.head.text) + 1\n",
    "            j_random_pos = random_value_except(random_numbers, j_pos)\n",
    "        \n",
    "            data_pos.append((input_ids, token_type_ids, tfidf_ids))\n",
    "            data_set1.append((i_pos, j_pos))\n",
    "            data_set2.append((i_pos, j_random_pos))\n",
    "            \n",
    "            if len(data_pos) % 20 == 0:\n",
    "                with open('data_pos.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_pos, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open('data_set1.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_set1, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open('data_set2.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_set2, f, pickle.HIGHEST_PROTOCOL)\n",
    "                print(len(data_pos))\n",
    "                \n",
    "            if len(data_pos) >= 2000:\n",
    "                break\n",
    "                \n",
    "            break\n",
    "                \n",
    "    if len(data_pos) >= 2000:\n",
    "        break\n",
    "            \n",
    "    for token in doc2:\n",
    "        if (token.text in tokens2_2) and (token.head.text in tokens2_2):\n",
    "            random_numbers = [ii+1 for ii in range(len(tokens1_1)+1, len(tokens1_1)+1+len(tokens1_2))]\n",
    "            i_pos = tokens2_2.index(token.text) + 2 + len(tokens2_1)\n",
    "            j_pos = tokens2_2.index(token.head.text) + 2 + len(tokens2_1)\n",
    "            j_random_pos = random_value_except(random_numbers, j_pos)\n",
    "        \n",
    "            data_pos.append((input_ids, token_type_ids, tfidf_ids))\n",
    "            data_set1.append((i_pos, j_pos))\n",
    "            data_set2.append((i_pos, j_random_pos))\n",
    "            \n",
    "            if len(data_pos) % 20 == 0:\n",
    "                with open('data_pos.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_pos, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open('data_set1.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_set1, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open('data_set2.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_set2, f, pickle.HIGHEST_PROTOCOL)\n",
    "                print(len(data_pos))\n",
    "                \n",
    "            if len(data_pos) >= 2000:\n",
    "                break\n",
    "        break\n",
    "\n",
    "    if len(data_pos) >= 2000:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21bba052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data_pos.pickle', 'wb') as f:\n",
    "    pickle.dump(data_pos, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('data_set1.pickle', 'wb') as f:\n",
    "    pickle.dump(data_set1, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('data_set2.pickle', 'wb') as f:\n",
    "    pickle.dump(data_set2, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0324d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
