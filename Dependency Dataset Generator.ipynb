{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608658f6",
   "metadata": {},
   "source": [
    "# 0. Load required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a12ce4-e456-41b5-ad2b-24f3dd1d255b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.46.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.3)\n",
      "Requirement already satisfied: jinja2 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.18.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/skhong/anaconda3/envs/jiant/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd8d5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForSequenceClassification\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2b1d0",
   "metadata": {},
   "source": [
    "# 1. Load tokenizer and dependency extraction module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0098e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokenizer = RobertaTokenizer('/home/skhong/jiant/roberta/wsc/models/roberta-base/tokenizer/vocab.json','/home/skhong/jiant/roberta/wsc/models/roberta-base/tokenizer/merges.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7f3945",
   "metadata": {},
   "source": [
    "# 2. Load Dataset\n",
    "- Preprocessed dataset used in this study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c35d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_file(\"/home/skhong/.cache/huggingface/datasets/super_glue/wsc/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/super_glue-train.arrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3339d",
   "metadata": {},
   "source": [
    "## 2.1 Dataset Structure\n",
    "- Since the dataset consists of two sentences, namely a premise and a hypothesis, there is an assumption that a [SEP] token is included between the two sentences. \n",
    "- Therefore, it is necessary to extract relationships between words within the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8688839a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(features: {'text': Value(dtype='string', id=None), 'span1_index': Value(dtype='int32', id=None), 'span2_index': Value(dtype='int32', id=None), 'span1_text': Value(dtype='string', id=None), 'span2_text': Value(dtype='string', id=None), 'idx': Value(dtype='int32', id=None), 'label': ClassLabel(num_classes=2, names=['False', 'True'], names_file=None, id=None)}, num_rows: 554)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25728be8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mark told Pete many lies about himself, which Pete included in his book. He should have been more skeptical.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d80fa743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['span2_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22081d73",
   "metadata": {},
   "source": [
    "# 3. Random_token_select Function\n",
    "- A function that randomly selects one token from tokens excluding tokens with dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1eadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_value_except(lst, excluded_value):\n",
    "    filtered_list = [item for item in lst if item != excluded_value]\n",
    "\n",
    "    if filtered_list:\n",
    "        random_value = random.choice(filtered_list)\n",
    "        return random_value\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6298c68c-ace9-40b2-a4c9-316e812df186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_list = stopwords.words(\"english\")\n",
    "\n",
    "def get_tfidf_vector(sentence,i):\n",
    "  # Join the tokenized words back into a sentence\n",
    "  if i==1:\n",
    "      vectorizer = TfidfVectorizer(stop_words=stop_words_list,norm=None)\n",
    "  elif i==2:\n",
    "      vectorizer = TfidfVectorizer(stop_words=stop_words_list,norm='l1')\n",
    "  elif i==3:\n",
    "     vectorizer = TfidfVectorizer(stop_words=stop_words_list)\n",
    "  tfidf_vector = vectorizer.fit_transform(sentence)\n",
    "    \n",
    "  # Return the TF-IDF vector\n",
    "  return tfidf_vector,vectorizer\n",
    "\n",
    "\n",
    "    \n",
    "def get_tfidf(tokenizer,input_ids,i):\n",
    "    def ids_to_string(ids):\n",
    "        ids = [ids]\n",
    "        lst = [v for v in ids if v != 0]\n",
    "        lst = [tokenizer._convert_id_to_token(i) for i in lst]\n",
    "        return lst    \n",
    "    temp = list(map(ids_to_string,input_ids))\n",
    "    sents = []\n",
    "    for sen in temp:\n",
    "        sents.append(' '.join(sen))\n",
    "        \n",
    "    tfidfvec,vec = get_tfidf_vector(sents,i)\n",
    "    tfidf_ids = []\n",
    "                \n",
    "    for _,sen in enumerate(sents):\n",
    "        tmp = []\n",
    "                    \n",
    "        for i,word in enumerate(sen.split(' ')):\n",
    "            if word in vec.vocabulary_.keys():\n",
    "                try:\n",
    "                    tmp.append(tfidfvec[i].toarray()[0][vec.vocabulary_[word]])\n",
    "                except:\n",
    "                    print(i,word)\n",
    "            else:\n",
    "                tmp.append(0)\n",
    "                        \n",
    "        tmp = list(map(round,tmp))\n",
    "                    \n",
    "                    \n",
    "        tmp += [0]*(512-len(tmp))\n",
    "        #tmp = [0]*(128)\n",
    "        assert len(tmp)==512, (tmp,len(tmp))\n",
    "        tfidf_ids.append(tmp)\n",
    "        #regularization_ids.append([0]*(128))\n",
    "    return tfidf_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60490e29",
   "metadata": {},
   "source": [
    "# 4. Dataset Generater\n",
    "- Compare tokenization results of two tokenizers, identify sentences with inter-token dependencies, extract token positions within those sentences, as well as positions of tokens without inter-token dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf937434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40690/3551756076.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(shuffle_index):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4134dab39c054297a49b5b816083c051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=554.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pos = []\n",
    "data_set1 = []\n",
    "data_set2 = []\n",
    "\n",
    "shuffle_index = [i for i in range(len(dataset['label']))]\n",
    "random.shuffle(shuffle_index)\n",
    "\n",
    "for i in tqdm_notebook(shuffle_index):    \n",
    "    y_temp = []\n",
    "    \n",
    "    text1 = dataset['text'][i]\n",
    "    doc1 = nlp(text1)\n",
    "    tokens1_1 = [d.text for d in doc1]\n",
    "    tokens2_1 = tokenizer.tokenize(text1)\n",
    "    \n",
    "    text2 = dataset['span1_text'][i] + \" \"+dataset['span2_text'][i]\n",
    "    doc2 = nlp(text2)\n",
    "    tokens1_2 = [d.text for d in doc2]\n",
    "    tokens2_2 = tokenizer.tokenize(text2)  \n",
    "    \n",
    "    encoded = tokenizer(text1, padding='max_length', max_length=512, truncation=True)\n",
    "    tfidf_ids = get_tfidf(tokenizer,encoded['input_ids'],3)\n",
    "    \n",
    "    input_ids = torch.Tensor([encoded['input_ids']]).type(torch.int32).cuda()\n",
    "    attention_mask = torch.Tensor([encoded['attention_mask']]).type(torch.int32).cuda()\n",
    "    tfidf_ids = torch.Tensor([tfidf_ids]).type(torch.int32).cuda()\n",
    "    \n",
    "    # \n",
    "    for token in doc1:\n",
    "        if (token.text in tokens2_1) and (token.head.text in tokens2_1):\n",
    "            random_numbers = [ii+1 for ii in range(len(tokens1_1))]\n",
    "            i_pos = tokens2_1.index(token.text) + 1\n",
    "            j_pos = tokens2_1.index(token.head.text) + 1\n",
    "            j_random_pos = random_value_except(random_numbers, j_pos)\n",
    "        \n",
    "            data_pos.append((input_ids, attention_mask, tfidf_ids))\n",
    "            data_set1.append((i_pos, j_pos))\n",
    "            data_set2.append((i_pos, j_random_pos))\n",
    "            \n",
    "            if len(data_pos) % 20 == 0:\n",
    "                with open('data_pos.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_pos, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open('data_set1.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_set1, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open('data_set2.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_set2, f, pickle.HIGHEST_PROTOCOL)\n",
    "                print(len(data_pos))\n",
    "                \n",
    "            if len(data_pos) >= 2000:\n",
    "                break\n",
    "                \n",
    "            break\n",
    "                \n",
    "    if len(data_pos) >= 2000:\n",
    "        break\n",
    "            \n",
    "    for token in doc2:\n",
    "        if (token.text in tokens2_2) and (token.head.text in tokens2_2):\n",
    "            random_numbers = [ii+1 for ii in range(len(tokens1_1)+1, len(tokens1_1)+1+len(tokens1_2))]\n",
    "            i_pos = tokens2_2.index(token.text) + 2 + len(tokens2_1)\n",
    "            j_pos = tokens2_2.index(token.head.text) + 2 + len(tokens2_1)\n",
    "            j_random_pos = random_value_except(random_numbers, j_pos)\n",
    "        \n",
    "            data_pos.append((input_ids, attention_mask, tfidf_ids))\n",
    "            data_set1.append((i_pos, j_pos))\n",
    "            data_set2.append((i_pos, j_random_pos))\n",
    "            \n",
    "            if len(data_pos) % 20 == 0:\n",
    "                with open('data_pos.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_pos, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open('data_set1.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_set1, f, pickle.HIGHEST_PROTOCOL)\n",
    "                with open('data_set2.pickle', 'wb') as f:\n",
    "                    pickle.dump(data_set2, f, pickle.HIGHEST_PROTOCOL)\n",
    "                print(len(data_pos))\n",
    "                \n",
    "            if len(data_pos) >= 2000:\n",
    "                break\n",
    "        break\n",
    "\n",
    "    if len(data_pos) >= 2000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21bba052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./validation/wsc/data_pos.pickle', 'wb') as f:\n",
    "    pickle.dump(data_pos, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('./validation/wsc/data_set1.pickle', 'wb') as f:\n",
    "    pickle.dump(data_set1, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open('./validation/wsc/data_set2.pickle', 'wb') as f:\n",
    "    pickle.dump(data_set2, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0324d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
